{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "Medical Image Classification Model Training Script<br>\n",
    "Training Script for Medical Image Classification<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# CHANGED: Importing from the new densenet_model file\n",
    "from densenet_model import create_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             roc_auc_score, roc_curve, f1_score, precision_score, \n",
    "                             recall_score, average_precision_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(data_dir, batch_size=32, img_size=224, use_weighted_sampler=False, num_workers=4):\n",
    "    \"\"\"\n",
    "    Load data and prepare for training/validation, with support for handling imbalanced data.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to data directory (must contain 'train' and 'val' folders).\n",
    "        batch_size: Batch size.\n",
    "        img_size: Image size after transformation.\n",
    "        use_weighted_sampler: Use WeightedRandomSampler to balance data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training image transformations (with enhanced data augmentation)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((int(img_size * 1.1), int(img_size * 1.1))),  # Resize slightly larger\n",
    "        transforms.RandomCrop(img_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),  # Good for medical images\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.1)  # Random Erasing technique\n",
    "    ])\n",
    "    \n",
    "    # Validation image transformations (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load training data\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(data_dir, 'train'),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    # Load validation data\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(data_dir, 'val'),\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Calculate weights to handle imbalanced data\n",
    "    train_sampler = None\n",
    "    class_weights = None\n",
    "    \n",
    "    if use_weighted_sampler:\n",
    "        # Calculate sample count for each class\n",
    "        class_counts = {}\n",
    "        for idx, (path, class_idx) in enumerate(train_dataset.samples):\n",
    "            class_counts[class_idx] = class_counts.get(class_idx, 0) + 1\n",
    "        \n",
    "        # Calculate weights (inverse of frequency)\n",
    "        total_samples = sum(class_counts.values())\n",
    "        num_classes = len(class_counts)\n",
    "        class_weights = [total_samples / (num_classes * count) for count in class_counts.values()]\n",
    "        \n",
    "        # Create weights for each sample\n",
    "        sample_weights = [class_weights[class_idx] for _, class_idx in train_dataset.samples]\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "        print(f\"âœ“ WeightedRandomSampler enabled\")\n",
    "        print(f\"  Weights: {dict(zip(train_dataset.classes, class_weights))}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),  # Do not shuffle if sampler is used\n",
    "        sampler=train_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Calculate weights for Weighted Loss if needed\n",
    "    if not class_weights:\n",
    "        class_counts = {}\n",
    "        for _, class_idx in train_dataset.samples:\n",
    "            class_counts[class_idx] = class_counts.get(class_idx, 0) + 1\n",
    "        total_samples = sum(class_counts.values())\n",
    "        num_classes = len(class_counts)\n",
    "        class_weights = [total_samples / (num_classes * count) for count in class_counts.values()]\n",
    "    \n",
    "    return train_loader, val_loader, train_dataset.classes, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Evaluate model on validation data and calculate all metrics\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Calculate probabilities (for AUC-ROC)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # AUC-ROC (for binary or multi-class classification)\n",
    "    if len(np.unique(all_labels)) == 2:\n",
    "        # Binary classification\n",
    "        if all_probs.shape[1] == 2:\n",
    "             auc_roc = roc_auc_score(all_labels, all_probs[:, 1])\n",
    "        else:\n",
    "             auc_roc = 0.0 # Edge case fallback\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        try:\n",
    "            auc_roc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
    "        except:\n",
    "            auc_roc = 0.0\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels, all_probs, {\n",
    "        'auc_roc': auc_roc,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path='training_history.png'):\n",
    "    \"\"\"Plot training history with all metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss Plot\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], label='Training Loss', marker='o')\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], label='Validation Loss', marker='s')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], label='Training Accuracy', marker='o')\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], label='Validation Accuracy', marker='s')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # AUC-ROC Plot (if available)\n",
    "    if 'val_auc_roc' in history:\n",
    "        axes[1, 0].plot(epochs, history['val_auc_roc'], label='Validation AUC-ROC', \n",
    "                        marker='s', color='green')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('AUC-ROC')\n",
    "        axes[1, 0].set_title('Validation AUC-ROC')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # F1-Score Plot (if available)\n",
    "    if 'val_f1_score' in history:\n",
    "        axes[1, 1].plot(epochs, history['val_f1_score'], label='Validation F1-Score', \n",
    "                        marker='s', color='purple')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('F1-Score')\n",
    "        axes[1, 1].set_title('Validation F1-Score')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    else:\n",
    "        axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Training history plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Number of Samples'})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Confusion matrix saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_probs, class_names, save_path='roc_curve.png'):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        # Binary classification\n",
    "        if y_probs.shape[1] == 2:\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "            auc_score = roc_auc_score(y_true, y_probs[:, 1])\n",
    "            plt.plot(fpr, tpr, label=f'{class_names[1]} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        for i in range(n_classes):\n",
    "            y_true_binary = (y_true == i).astype(int)\n",
    "            # Check if class exists in batch to avoid errors\n",
    "            if np.sum(y_true_binary) > 0:\n",
    "                fpr, tpr, _ = roc_curve(y_true_binary, y_probs[:, i])\n",
    "                try:\n",
    "                    auc_score = roc_auc_score(y_true_binary, y_probs[:, i])\n",
    "                    plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ROC curve saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ==================== Settings ====================\n",
    "    DATA_DIR = 'data'  # Path to data folder\n",
    "    \n",
    "    # CHANGED: Reduced batch size slightly for DenseNet safety (optional, can be 64 if GPU is good)\n",
    "    BATCH_SIZE = 32 \n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 0.001\n",
    "    IMG_SIZE = 224\n",
    "    NUM_CLASSES = None  # Automatically detected from data\n",
    "    SAVE_DIR = 'checkpoints'\n",
    "    \n",
    "    # Transfer Learning Settings\n",
    "    USE_TRANSFER_LEARNING = True \n",
    "    # CHANGED: Set model name to densenet121\n",
    "    MODEL_NAME = 'densenet121'  # 'resnet18', 'resnet50', 'vgg16', 'densenet121', 'inception_v3'\n",
    "    FREEZE_BACKBONE = True  # Freeze backbone initially\n",
    "    AUTO_UNFREEZE = True\n",
    "    UNFREEZE_EPOCH = 5  # Unfreeze backbone after this epoch\n",
    "    PRETRAINED = True \n",
    "    \n",
    "    # Imbalanced Data Settings\n",
    "    USE_WEIGHTED_SAMPLER = True \n",
    "    USE_WEIGHTED_LOSS = True \n",
    "    \n",
    "    # Learning Rate Scheduler Settings\n",
    "    USE_SCHEDULER = True\n",
    "    SCHEDULER_TYPE = 'cosine'  # 'plateau', 'cosine', 'step'\n",
    "    \n",
    "    # ==================== Initialize Directories ====================\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    # GPU Check\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if AUTO_UNFREEZE and UNFREEZE_EPOCH > NUM_EPOCHS:\n",
    "        UNFREEZE_EPOCH = NUM_EPOCHS\n",
    "    \n",
    "    # ==================== Load Data ====================\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, class_names, class_weights = get_data_loaders(\n",
    "        DATA_DIR,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        img_size=IMG_SIZE,\n",
    "        use_weighted_sampler=USE_WEIGHTED_SAMPLER,\n",
    "        num_workers=4\n",
    "    )\n",
    "    print(f\"\\nâœ“ Data loaded successfully!\")\n",
    "    print(f\"  Classes: {class_names}\")\n",
    "    print(f\"  Training images: {len(train_loader.dataset)}\")\n",
    "    print(f\"  Validation images: {len(val_loader.dataset)}\")\n",
    "    \n",
    "    detected_num_classes = len(class_names)\n",
    "    if NUM_CLASSES is None:\n",
    "        NUM_CLASSES = detected_num_classes\n",
    "    elif NUM_CLASSES != detected_num_classes:\n",
    "        print(f\"\\nâڑ ï¸ڈ  NUM_CLASSES ({NUM_CLASSES}) matches detected classes \"\n",
    "              f\"({detected_num_classes}). Using data-detected value.\")\n",
    "        NUM_CLASSES = detected_num_classes\n",
    "    print(f\"  Total classes: {NUM_CLASSES}\")\n",
    "    \n",
    "    # ==================== Create Model ====================\n",
    "    print(f\"\\nCreating model...\")\n",
    "    if USE_TRANSFER_LEARNING:\n",
    "        print(f\"  Model Type: Transfer Learning ({MODEL_NAME})\")\n",
    "        model = create_model(\n",
    "            num_classes=NUM_CLASSES,\n",
    "            input_channels=3,\n",
    "            pretrained=PRETRAINED,\n",
    "            model_type='transfer',\n",
    "            model_name=MODEL_NAME,\n",
    "            freeze_backbone=FREEZE_BACKBONE\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  Model Type: Custom CNN (From Scratch)\")\n",
    "        model = create_model(\n",
    "            num_classes=NUM_CLASSES,\n",
    "            input_channels=3,\n",
    "            pretrained=False,\n",
    "            model_type='custom'\n",
    "        )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    backbone_unfrozen = not FREEZE_BACKBONE\n",
    "    if FREEZE_BACKBONE:\n",
    "        print(\"  Backbone layers initially frozen.\")\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # ==================== Loss Function ====================\n",
    "    if USE_WEIGHTED_LOSS and class_weights:\n",
    "        weights = torch.FloatTensor(class_weights).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        print(f\"\\nâœ“ Weighted Loss enabled\")\n",
    "        print(f\"  Weights: {dict(zip(class_names, class_weights))}\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # ==================== Optimizer ====================\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    \n",
    "    # ==================== Learning Rate Scheduler ====================\n",
    "    scheduler = None\n",
    "    if USE_SCHEDULER:\n",
    "        if SCHEDULER_TYPE == 'plateau':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=5\n",
    "            )\n",
    "        elif SCHEDULER_TYPE == 'cosine':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=NUM_EPOCHS, eta_min=1e-6\n",
    "            )\n",
    "        elif SCHEDULER_TYPE == 'step':\n",
    "            scheduler = optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=15, gamma=0.5\n",
    "            )\n",
    "        print(f\"\\nâœ“ Learning Rate Scheduler enabled ({SCHEDULER_TYPE})\")\n",
    "    \n",
    "    # ==================== Training History ====================\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_auc_roc': [],\n",
    "        'val_f1_score': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_val_auc = 0.0\n",
    "    \n",
    "    # ==================== Training Loop ====================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Starting Training...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if FREEZE_BACKBONE and AUTO_UNFREEZE and not backbone_unfrozen and (epoch + 1) >= UNFREEZE_EPOCH:\n",
    "            print(f\"--> Epoch {epoch+1}: Unfreezing backbone layers.\")\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "            backbone_unfrozen = True\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_preds, val_labels, val_probs, metrics = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Update LR\n",
    "        if scheduler:\n",
    "            if SCHEDULER_TYPE == 'plateau':\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Save History\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc_roc'].append(metrics['auc_roc'])\n",
    "        history['val_f1_score'].append(metrics['f1_score'])\n",
    "        history['val_precision'].append(metrics['precision'])\n",
    "        history['val_recall'].append(metrics['recall'])\n",
    "        \n",
    "        # Print Results\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Val AUC-ROC: {metrics['auc_roc']:.4f}, Val F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"Val Precision: {metrics['precision']:.4f}, Val Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save Best Model\n",
    "        save_model = False\n",
    "        if metrics['auc_roc'] > best_val_auc:\n",
    "            best_val_auc = metrics['auc_roc']\n",
    "            save_model = True\n",
    "            print(f\"âœ“ AUC-ROC improved! (AUC: {best_val_auc:.4f})\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_model = True\n",
    "            print(f\"âœ“ Accuracy improved! (Acc: {best_val_acc:.4f})\")\n",
    "        \n",
    "        if save_model:\n",
    "            # CHANGED: Saving as densenet\n",
    "            save_path = os.path.join(SAVE_DIR, 'best_densenet121_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_auc_roc': metrics['auc_roc'],\n",
    "                'class_names': class_names,\n",
    "                'model_type': 'transfer' if USE_TRANSFER_LEARNING else 'custom',\n",
    "                'model_name': MODEL_NAME if USE_TRANSFER_LEARNING else 'custom'\n",
    "            }, save_path)\n",
    "            print(f\"  Model saved to {save_path}\")\n",
    "    \n",
    "    # ==================== Final Evaluation ====================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Final Evaluation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_labels, val_preds, target_names=class_names))\n",
    "    \n",
    "    # Plotting\n",
    "    print(\"\\nPlotting graphs...\")\n",
    "    plot_training_history(history, save_path=os.path.join(SAVE_DIR, 'training_history.png'))\n",
    "    plot_confusion_matrix(val_labels, val_preds, class_names, \n",
    "                          save_path=os.path.join(SAVE_DIR, 'confusion_matrix.png'))\n",
    "    plot_roc_curve(val_labels, val_probs, class_names, \n",
    "                   save_path=os.path.join(SAVE_DIR, 'roc_curve.png'))\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Final Result Summary:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Best Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Best AUC-ROC: {best_val_auc:.4f}\")\n",
    "    print(f\"Final F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"Final Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Final Recall: {metrics['recall']:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    # CHANGED: Updated filename in print statement\n",
    "    print(f\"\\nâœ“ Training completed successfully!\")\n",
    "    print(f\"  Best model saved: {os.path.join(SAVE_DIR, 'best_densenet121_model.pth')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WhatsApp Image 2025-12-30 at 1.39.29 PM (1).jpeg](<attachment:WhatsApp Image 2025-12-30 at 1.39.29 PM (1).jpeg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eğitim Sonuçlarının Analizi**\n",
    "\n",
    "Eğitim sürecine ait grafikleri incelediğimizde şu sonuçları görüyoruz:\n",
    "\n",
    "* **Loss (Kayıp) Grafiği:** Hem Training hem de Validation loss değerleri düzenli bir şekilde düşüyor. En önemlisi, iki çizgi birbirine yakın seyrediyor; bu da modelin ezberlemediğini (overfitting yapmadığını) ve veriyi gerçekten öğrendiğini gösteriyor.\n",
    "* **Accuracy (Doğruluk) Grafiği:** Modelimizin başarısı her epoch'ta artarak devam etmiş ve sonuçta **%95** civarında yüksek bir doğruluk oranına ulaşmıştır. Grafikteki dalgalanmaların azalması, eğitimin istikrarlı bir şekilde tamamlandığını işaret ediyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<WhatsApp Image 2025-12-30 at 1.39.29 PM-1.jpeg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Confusion Matrix (Hata Matrisi) Değerlendirmesi**\n",
    "\n",
    "Modelin hangi sınıflarda ne kadar başarılı olduğunu daha detaylı görmek için Confusion Matrix'i inceledik:\n",
    "\n",
    "* **Genel Başarı:** Matrisin köşegeni (diagonal) üzerindeki koyu renkli kutular, doğru tahmin sayılarımızın yüksek olduğunu gösteriyor.\n",
    "* **Sınıf Bazlı Analiz:**\n",
    "    * **Tüberküloz** ve **Normal** sınıflarındaki tüm görüntüleri hatasız (%100) bilmişiz.\n",
    "    * Hata payımız oldukça düşük; sadece **1 COVID-19** vakasını Tüberküloz ile, ve **1 Pnömoni** vakasını Normal ile karıştırmış.\n",
    "* **Sonuç:** Toplamda sadece 2 yanlış tahminimiz var. Bu da geliştirdiğimiz DenseNet modelinin medikal sınıflandırma için gayet güvenilir çalıştığını kanıtlıyor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
